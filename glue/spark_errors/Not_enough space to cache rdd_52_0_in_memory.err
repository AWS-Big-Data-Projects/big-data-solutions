Error StackTrace ==>>>




      20/11/12 00:28:17 INFO MemoryStore: Will not store rdd_12_1455
      20/11/12 00:28:17 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KB for computing block rdd_12_1455 in memory.
      20/11/12 00:28:17 WARN MemoryStore: Not enough space to cache rdd_12_1455 in memory! (computed 0.0 B so far)
      20/11/12 00:28:17 INFO MemoryStore: Memory use = 2.8 GB (blocks) + 1024.0 KB (scratch space shared across 1 tasks(s)) = 2.8 GB. Storage limit = 2.8 GB.
      20/11/12 00:28:17 WARN BlockManager: Persisting block rdd_12_1455 to disk instead.

      20/11/12 00:40:24 ERROR ApplicationMaster: RECEIVED SIGNAL TERM
      
===============================
Why this error usually occurs ?
===============================

The root cause for the above is spark cache's the RDD and it doesn't have enough memory to continue processing data. The above StackTrace tell us there is not enough memory on the executor available to be used for the RDD storage. i.e. only fraction of memory is used from the actual executor memory available.So be aware that not the whole amount of driver memory will be available for RDD storage.Spark's executor memory usually controls calculating the amount to dedicate to Spark's memory cache.

Executor memory includes memory required for executing the tasks plus overhead memory which should not be greater than the size of JVM and yarn maximum container size. Even though it is a Warning but after waiting for some time spark part of the task tries to execute again but it runs into the same problem again and leading to task have been waiting for finally suspended. It clearly tells above the Storage limit = 2.8 GB.


20/11/12 00:40:24 ERROR ApplicationMaster: RECEIVED SIGNAL TERM

    SIGTERM is usually received due to memory over-utilization, using the bigger Worker types,  will let you have much larger memory allocation for them.

    If you see , the job is running with a single executor for most of its lifetime, peaking at only as an example 5 or 6 executors at some point. 

    For reference, with the 10 DPUs and the Standard worker type you configure for your job, you should be getting a maximum of 17 executors - which means your resources are being wasted. This also explains why your executors are running out of memory - instead of distributing the dataset amongst them, only a few are trying to load it entirely and failing to do so.

    This proves the reads are not parallelized. I would recommend following our documentation on  capacity planning section [2]. Doing proper partitioning should divide the dataset across all of your executors, which will decrease memory pressure on them, and in the end make your job run properly.
    
    For now I would suggest simply making sure there's parallel reads, which should get you a nice number of executors at the start. Then the default number of partitions should keep that number, so you can try to apply the changes stated in the documentation link mentioned before and run again to see how it goes. If the job fails again, check the executor count metric. If it goes low again you'll have to check your code to see if there's any step at which the partition count can be going low (most likely a join) and you can repartition after it. 

    Also if the metric i.e. metric 'glue.driver.ExecutorAllocationManager.executors.numberAllExecutors' reporting a value lower than what is set for Max capacity , there's a partitioning issue. There's several reasons as to why this could be happening, but the easiest way to address it typically is to add a repartition call after your read operation. The number of partitions should be 4 times the number of executors you can have. If not, your job is partitioning properly and you will need to provision more resources (workers) to handle the amount of data. In order to understand how many you can check the 'glue.driver.ExecutorAllocationManager.executors.numberMaxNeededExecutors' metric, which will tell you how many executors are needed to process your dataset with maximum parallelism as I explained above.
    
    
    References :
[1] https://docs.aws.amazon.com/glue/latest/dg/add-job.html
[2] https://docs.aws.amazon.com/glue/latest/dg/monitor-debug-capacity.html
